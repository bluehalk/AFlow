 #!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
MBPPæ•°æ®é›†å¤ç°è„šæœ¬

ç”¨äºå¤ç°MBPP (Mostly Basic Python Problems) æ•°æ®é›†çš„å®éªŒç»“æœã€‚
è¯¥è„šæœ¬ä½¿ç”¨AFlowæ¡†æ¶ä¸­çš„Graphæ–¹æ³•æ¥è§£å†³Pythonç¼–ç¨‹é—®é¢˜ã€‚
"""

import asyncio
import json
import os
import sys
import time
from pathlib import Path
from typing import Dict, List, Tuple, Any

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from benchmarks.mbpp import MBPPBenchmark
from scripts.async_llm import LLMsConfig

# å¯¼å…¥Graphå·¥ä½œæµ
from data.results.results.MBPP.graphs_test.round_14.graph import Workflow

def load_dataset(dataset_path: str) -> List[Dict]:
    """åŠ è½½MBPPæ•°æ®é›†"""
    data = []
    with open(dataset_path, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                data.append(json.loads(line))
    return data

class MBPPExperiment:
    """MBPPå®éªŒç±»"""
    
    def __init__(self, model_name: str = "gpt-4o-mini"):
        self.model_name = model_name
        self.llm_config = self._get_llm_config()
        self.benchmark = MBPPBenchmark(
            name="MBPP",
            file_path="data/datasets",
            log_path="logs"
        )
        
    def _get_llm_config(self):
        """è·å–LLMé…ç½®"""
        models_config = LLMsConfig.default()
        llm_config = models_config.get(self.model_name)
        if llm_config is None:
            raise ValueError(f"Model '{self.model_name}' not found in configuration")
        return llm_config
    
    async def run_single_problem(self, problem_data: Dict) -> Tuple[str, str, str, float, float]:
        """è¿è¡Œå•ä¸ªé—®é¢˜"""
        # åˆ›å»ºå·¥ä½œæµå®ä¾‹
        workflow = Workflow(
            name="MBPP_Graph_Workflow",
            llm_config=self.llm_config,
            dataset="MBPP"
        )
        
        # ä½¿ç”¨benchmarkçš„evaluate_problemæ–¹æ³•
        return await self.benchmark.evaluate_problem(problem_data, workflow)
    
    async def run_experiment(self, dataset_path: str, num_samples: int = None, output_path: str = None):
        """è¿è¡Œå®Œæ•´å®éªŒ"""
        print(f"ğŸš€ å¼€å§‹MBPPæ•°æ®é›†å®éªŒ...")
        print(f"ğŸ“Š ä½¿ç”¨æ¨¡å‹: {self.model_name}")
        print(f"ğŸ“ æ•°æ®é›†è·¯å¾„: {dataset_path}")
        
        # åŠ è½½æ•°æ®é›†
        data = load_dataset(dataset_path)
        
        if num_samples:
            data = data[:num_samples]
            print(f"ğŸ”¢ é™åˆ¶æ ·æœ¬æ•°é‡: {num_samples}")
        
        print(f"ğŸ“‹ æ€»å…±é—®é¢˜æ•°é‡: {len(data)}")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        if output_path is None:
            timestamp = time.strftime("%Y%m%d_%H%M%S")
            output_path = f"mbpp_results_{timestamp}.csv"
        
        os.makedirs(os.path.dirname(output_path) if os.path.dirname(output_path) else ".", exist_ok=True)
        
        # è¿è¡Œå®éªŒ
        results = []
        total_cost = 0.0
        correct_count = 0
        
        print("\n" + "="*80)
        print("å¼€å§‹å¤„ç†é—®é¢˜...")
        print("="*80)
        
        for i, problem in enumerate(data):
            print(f"\nğŸ“ å¤„ç†é—®é¢˜ {i+1}/{len(data)}: {problem.get('entry_point', 'Unknown')}")
            
            try:
                # è¿è¡Œå•ä¸ªé—®é¢˜
                input_text, prediction, expected_output, score, cost = await self.run_single_problem(problem)
                
                results.append({
                    'question': input_text,
                    'prediction': prediction,
                    'expected_output': expected_output,
                    'score': score,
                    'cost': cost
                })
                
                total_cost += cost
                if score > 0:
                    correct_count += 1
                
                print(f"âœ… ç»“æœ: {'é€šè¿‡' if score > 0 else 'å¤±è´¥'}")
                print(f"ğŸ’° æˆæœ¬: ${cost:.6f}")
                print(f"ğŸ“Š å½“å‰å‡†ç¡®ç‡: {correct_count}/{i+1} ({correct_count/(i+1)*100:.2f}%)")
                
            except Exception as e:
                print(f"âŒ é”™è¯¯: {str(e)}")
                results.append({
                    'question': problem.get('prompt', ''),
                    'prediction': f"Error: {str(e)}",
                    'expected_output': '',
                    'score': 0.0,
                    'cost': 0.0
                })
        
        # ä¿å­˜ç»“æœ
        self.save_results(results, output_path)
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        self.print_statistics(results, total_cost, correct_count, len(data))
        
        return results
    
    def save_results(self, results: List[Dict], output_path: str):
        """ä¿å­˜ç»“æœåˆ°CSVæ–‡ä»¶"""
        import csv
        
        # è·å–åˆ—å
        columns = self.benchmark.get_result_columns()
        
        # æ˜ å°„ç»“æœå­—æ®µåˆ°CSVåˆ—
        field_mapping = {
            'question': 'inputs',
            'prediction': 'prediction', 
            'expected_output': 'expected_output',
            'score': 'score',
            'cost': 'cost'
        }
        
        with open(output_path, 'w', newline='', encoding='utf-8') as csvfile:
            writer = csv.writer(csvfile)
            writer.writerow(columns)
            
            for result in results:
                row = []
                for col in columns:
                    # æ‰¾åˆ°å¯¹åº”çš„ç»“æœå­—æ®µ
                    field_name = None
                    for result_field, csv_col in field_mapping.items():
                        if csv_col == col:
                            field_name = result_field
                            break
                    
                    if field_name and field_name in result:
                        row.append(result[field_name])
                    else:
                        row.append('')
                
                writer.writerow(row)
        
        print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
    
    def print_statistics(self, results: List[Dict], total_cost: float, correct_count: int, total_count: int):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        accuracy = correct_count / total_count * 100 if total_count > 0 else 0
        avg_cost = total_cost / total_count if total_count > 0 else 0
        
        print("\n" + "="*80)
        print("ğŸ“Š å®éªŒç»Ÿè®¡ç»“æœ")
        print("="*80)
        print(f"ğŸ¯ æ€»å‡†ç¡®ç‡: {correct_count}/{total_count} ({accuracy:.2f}%)")
        print(f"ğŸ’° æ€»æˆæœ¬: ${total_cost:.6f}")
        print(f"ğŸ’¸ å¹³å‡æˆæœ¬: ${avg_cost:.6f}")
        print(f"ğŸ”§ ä½¿ç”¨æ¨¡å‹: {self.model_name}")
        print("="*80)

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="MBPPæ•°æ®é›†å¤ç°å®éªŒ")
    parser.add_argument(
        "--dataset", 
        type=str, 
        choices=["test", "validate", "public_test"],
        default="validate",
        help="é€‰æ‹©æ•°æ®é›†ç±»å‹ (default: validate)"
    )
    parser.add_argument(
        "--model", 
        type=str, 
        default="gpt-4o-mini",
        help="LLMæ¨¡å‹åç§° (default: gpt-4o-mini)"
    )
    parser.add_argument(
        "--num_samples", 
        type=int, 
        help="é™åˆ¶æ ·æœ¬æ•°é‡ (ä¸æŒ‡å®šåˆ™ä½¿ç”¨å…¨éƒ¨æ ·æœ¬)"
    )
    parser.add_argument(
        "--output", 
        type=str,
        help="è¾“å‡ºæ–‡ä»¶è·¯å¾„ (default: è‡ªåŠ¨ç”Ÿæˆ)"
    )
    
    args = parser.parse_args()
    
    # ç¡®å®šæ•°æ®é›†è·¯å¾„
    dataset_paths = {
        "test": "data/datasets/mbpp_test.jsonl",
        "validate": "data/datasets/mbpp_validate.jsonl", 
        "public_test": "data/datasets/mbpp_public_test.jsonl"
    }
    
    dataset_path = dataset_paths[args.dataset]
    
    if not os.path.exists(dataset_path):
        print(f"âŒ æ•°æ®é›†æ–‡ä»¶ä¸å­˜åœ¨: {dataset_path}")
        print("è¯·ç¡®ä¿æ•°æ®é›†æ–‡ä»¶å­˜åœ¨äºæ­£ç¡®çš„è·¯å¾„ã€‚")
        return
    
    print("ğŸ‰ MBPPæ•°æ®é›†å¤ç°å®éªŒ")
    print("="*50)
    print("ğŸ“– å…³äºMBPPæ•°æ®é›†:")
    print("  - MBPP (Mostly Basic Python Problems)")
    print("  - åŒ…å«Pythonç¼–ç¨‹é—®é¢˜å’Œæµ‹è¯•ç”¨ä¾‹")
    print("  - æ¯ä¸ªé—®é¢˜åŒ…å«: prompt, code, test, entry_pointç­‰å­—æ®µ")
    print("  - Graphæ–¹æ³•åŒ…å«: ä»£ç ç”Ÿæˆ â†’ è‡ªä¸€è‡´æ€§é›†æˆ â†’ æµ‹è¯•éªŒè¯ â†’ é”™è¯¯ä¿®å¤")
    print("="*50)
    
    # åˆ›å»ºå®éªŒå®ä¾‹å¹¶è¿è¡Œ
    experiment = MBPPExperiment(model_name=args.model)
    
    try:
        asyncio.run(experiment.run_experiment(
            dataset_path=dataset_path,
            num_samples=args.num_samples,
            output_path=args.output
        ))
    except KeyboardInterrupt:
        print("\nâš ï¸  å®éªŒè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ å®éªŒè¿è¡Œå‡ºé”™: {str(e)}")
        raise

if __name__ == "__main__":
    main()