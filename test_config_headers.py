#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import asyncio
from scripts.async_llm import LLMsConfig, create_llm_instance

async def test_config():
    """æµ‹è¯•é…ç½®æ˜¯å¦æ­£ç¡®åŠ è½½extra_headers"""
    
    print("ğŸ”§ æµ‹è¯•é…ç½®åŠ è½½...")
    
    # åŠ è½½é»˜è®¤é…ç½®
    config = LLMsConfig.default()
    
    # è·å–meta-llama/llama-3.3-70b-instruct:freeé…ç½®
    llm_config = config.get('meta-llama/llama-3.3-70b-instruct:free')
    
    print(f"æ¨¡å‹: {llm_config.model}")
    print(f"Base URL: {llm_config.base_url}")
    print(f"API Key: {llm_config.key[:20]}..." if llm_config.key else "No API Key")
    print(f"Extra Headers: {llm_config.extra_headers}")
    print(f"Temperature: {llm_config.temperature}")
    
    # åˆ›å»ºLLMå®ä¾‹
    print("\nğŸš€ åˆ›å»ºLLMå®ä¾‹...")
    llm = create_llm_instance(llm_config)
    
    print(f"LLMé…ç½®çš„extra_headers: {llm.config.extra_headers}")
    
    # æµ‹è¯•ä¸€ä¸ªç®€å•çš„è°ƒç”¨
    print("\nğŸ’¬ æµ‹è¯•ç®€å•è°ƒç”¨...")
    try:
        result = await llm("What is 2+2? Answer only with the number.")
        print(f"âœ… æµ‹è¯•æˆåŠŸ!")
        print(f"å“åº”: {result}")
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
    
    return llm.get_usage_summary()

if __name__ == "__main__":
    summary = asyncio.run(test_config())
    print(f"\nğŸ“Š ä½¿ç”¨æ‘˜è¦: {summary}") 