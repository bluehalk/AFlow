#!/usr/bin/env python3
# -*- coding: utf-8 -*-
import asyncio
import json
import os
import sys
import time
import pandas as pd
from pathlib import Path
from typing import Literal, Dict, List, Any
from datetime import datetime
import statistics

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from scripts.operators import Custom, ScEnsemble, Programmer, run_code
from scripts.async_llm import LLMsConfig, create_llm_instance
from benchmarks.gsm8k import GSM8KBenchmark
import concurrent.futures

DatasetType = Literal["HumanEval", "MBPP", "GSM8K", "MATH", "HotpotQA", "DROP"]

PYTHON_CODE_VERIFIER_PROMPT = """
You are a professional Python programmer. Your task is to write complete, self-contained code based on a given mathematical problem and output the answer. 
The code should include all necessary imports and dependencies, and be ready to run without additional setup or environment configuration.
Problem description: {problem}
Other analysis: {analysis}
{feedback}

Your code should:
1. Implement the calculation steps described in the problem.
2. Define a function named `solve` that performs the calculation and returns the result. 
The `solve` function should not require any input parameters; 
instead, it should obtain all necessary inputs from within the function or from globally defined variables.
3. `solve` function return the final calculation result.

Please ensure your code is efficient, well-commented, and follows Python best practices. 
The output should be limited to basic data types such as strings, integers, and floats. 
It is prohibited to transmit images or other file formats. The code output is intended for a text-based language model.
"""

SC_ENSEMBLE_PROMPT = """
Given the question described as follows: {question}
Several solutions have been generated to address the given question. They are as follows:
{solutions}

Carefully evaluate these solutions and identify the answer that appears most frequently across them. 
This consistency in answers is crucial for determining the most reliable solution.

In the "thought" field, provide a detailed explanation of your thought process. 
In the "solution_letter" field, output only the single letter ID (A, B, C, etc.) corresponding to the most consistent solution.
 Do not include any additional text or explanation in the "solution_letter" field.
"""


# ä»round 10è·å–çš„æœ€ä¼˜prompt (æœ€ä½³æ€§èƒ½)
OPTIMAL_MATH_SOLVE_PROMPT = """You are a highly skilled mathematician tasked with solving a math problem. Follow these steps carefully:

1. Read and understand the problem thoroughly.
2. Identify all key information, variables, and relationships.
3. Determine the appropriate mathematical concepts, formulas, or equations to use.
4. Solve the problem step-by-step, showing all your work clearly.
5. Double-check your calculations and reasoning at each step.
6. Provide a clear and concise final answer.
7. Verify your solution by plugging it back into the original problem or using an alternative method if possible.

Format your answer as follows:
- Use LaTeX notation for mathematical expressions where appropriate.
- Show each step of your solution process clearly.
- Clearly state your final answer at the end of your solution.
- Express numerical answers as precise values (avoid rounding unless specified).
- Ensure that your final answer is a single numerical value without any units or additional text.
- Do not include any explanatory text with your final answer, just the number itself.

For example, if the final answer is 42.5, your response should end with just:
42.5

Here's the problem to solve:

"""

# ç»¼åˆçš„overall_promptï¼Œå°†workflowçš„æ‰€æœ‰æ­¥éª¤æ•´åˆåˆ°ä¸€ä¸ªpromptä¸­
OVERALL_WORKFLOW_PROMPT = """
You are an expert mathematician with advanced problem-solving capabilities. 
You will solve a math problem using a comprehensive multi-approach workflow. Follow these steps carefully:

## STEP 1: GENERATE MULTIPLE SOLUTIONS
First, generate THREE different solutions to the problem using different approaches or perspectives:

**Solution A (Direct Mathematical Approach):**
- Read and understand the problem thoroughly
- Identify all key information, variables, and relationships
- Determine the appropriate mathematical concepts, formulas, or equations
- Solve step-by-step, showing all work clearly
- Double-check calculations and reasoning

**Solution B (Alternative Mathematical Approach):**
- Take a different mathematical perspective or method
- Use alternative formulas or calculation strategies if possible
- Show complete step-by-step reasoning
- Verify calculations independently

**Solution C (Verification-Focused Approach):**
- Solve using a third method or approach
- Focus on verification and cross-checking
- Use estimation or alternative calculation methods to validate
- Ensure logical consistency

## STEP 2: SELF-CONSISTENCY EVALUATION
Carefully evaluate the three solutions above:
- Compare the final answers from all three solutions
- Identify which answer appears most frequently (consistency check)
- Analyze the reasoning quality and mathematical rigor of each solution
- If there are discrepancies, determine which solution has the most reliable approach
- Select the most consistent and mathematically sound solution

## STEP 3: PYTHON CODE VERIFICATION
Write Python code to verify your selected solution:
- Implement the calculation steps described in your selected solution
- Create a function named `solve()` that performs the calculation and returns the result
- The function should be self-contained with all necessary inputs defined within
- Include clear comments explaining each calculation step
- Execute the logic to verify your mathematical solution

## STEP 4: FINAL ANSWER
Based on your multi-approach analysis and code verification:
- If the code output matches your mathematical solution, use that result
- If there's a discrepancy, carefully review and use the most reliable approach
- Provide your final answer as a single numerical value
- Do NOT include units, additional text, or explanations with the final answer

## FORMAT YOUR RESPONSE AS:

### Solution A:
[Your first mathematical solution here]

### Solution B:
[Your second mathematical solution here]

### Solution C:
[Your third mathematical solution here]

### Self-Consistency Evaluation:
[Compare the three solutions and select the most consistent one]

### Python Code Verification:
```python
def solve():
    # [Your verification code here]
    # [Include clear comments]
    return result

# Execute and show result
answer = solve()
print(answer)
```

### Final Answer:
[Single numerical value only]

Now solve this problem:

"""

class IO_with_optimal_prompt:
    """ç®€å•çš„è¾“å…¥è¾“å‡ºæ¨¡å¼ï¼Œä½¿ç”¨æœ€ä¼˜prompt"""
    def __init__(self, name: str, llm_config, dataset: str, max_retries: int = 3):
        self.name = name
        self.llm_config = llm_config
        self.dataset = dataset
        self.max_retries = max_retries
        self.llm = create_llm_instance(llm_config)
        self.custom = Custom(self.llm)
    
    async def _call_with_simple_retry(self, problem: str):
        """é‡è¯•æœºåˆ¶çš„å†…éƒ¨æ–¹æ³•"""
        for attempt in range(self.max_retries):
            try:
                if attempt > 0:
                    await asyncio.sleep(0.1)  # é‡è¯•æ—¶çŸ­æš‚å»¶è¿Ÿ
                result = await self.custom(input=problem, instruction=OPTIMAL_MATH_SOLVE_PROMPT)
                return result['response'], self.llm.get_usage_summary()["total_cost"]
            except Exception as e:
                error_msg = str(e).lower()
                if any(keyword in error_msg for keyword in ['connection', 'timeout', 'network', 'rate limit']):
                    if attempt < self.max_retries - 1:
                        print(f"    âš ï¸  APIé”™è¯¯ï¼Œé‡è¯•ä¸­: {e}")
                        continue
                raise e
        raise Exception("æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†")
    
    async def __call__(self, problem: str):
        """ä¸»è¦çš„è°ƒç”¨æ–¹æ³•"""
        print(f"  IO_with_optimal_prompt: å¤„ç†é—®é¢˜...")
        return await self._call_with_simple_retry(problem)


class OptimalWorkflow:
    def __init__(self, name: str, llm_config, dataset: DatasetType, max_retries: int = 3) -> None:
        self.name = name
        self.dataset = dataset
        self.llm_config = llm_config
        self.max_retries = max_retries
        self.llm = create_llm_instance(llm_config)
        self.custom = Custom(self.llm)
        self.sc_ensemble = ScEnsemble(self.llm)
        self.programmer = Programmer(self.llm)

    async def _call_with_simple_retry(self, func, *args, **kwargs):
        """ç®€å•é‡è¯•è¾…åŠ©å‡½æ•° - å»æ‰å»¶è¿Ÿç‰ˆæœ¬"""
        for attempt in range(self.max_retries):
            try:
                if attempt > 0:
                    await asyncio.sleep(0.1)  # é‡è¯•æ—¶åªéœ€å¾ˆçŸ­å»¶è¿Ÿ
                return await func(*args, **kwargs)
            except Exception as e:
                error_msg = str(e).lower()
                if any(keyword in error_msg for keyword in ['connection', 'timeout', 'network', 'rate limit']):
                    if attempt < self.max_retries - 1:
                        print(f"    âš ï¸  APIé”™è¯¯ï¼Œé‡è¯•ä¸­: {e}")
                        continue
                raise e

    async def __call__(self, problem: str):
        """æœ€ä¼˜å·¥ä½œæµå®ç° - æ— å»¶è¿Ÿç‰ˆæœ¬"""
        solutions = []
        
        # ç”Ÿæˆ3ä¸ªè§£å†³æ–¹æ¡ˆ - å®Œå…¨å¹¶å‘æ‰§è¡Œ
        print(f"  OptimalWorkflow: å¹¶å‘ç”Ÿæˆ3ä¸ªè§£å†³æ–¹æ¡ˆ...")
        tasks = []
        for i in range(3):
            task = self._call_with_simple_retry(
                self.custom, input=problem, instruction=OPTIMAL_MATH_SOLVE_PROMPT
            )
            tasks.append(task)
        
        # å®Œå…¨å¹¶å‘æ‰§è¡Œï¼Œæ— å»¶è¿Ÿ
        try:
            # ä½¿ç”¨gatherè¿›è¡ŒçœŸæ­£çš„å¹¶å‘æ‰§è¡Œ
            solutions = await asyncio.gather(*tasks)
            
            # æå–å“åº”
            solution_texts = [sol['response'] for sol in solutions]
            
        except Exception as e:
            print(f"    âŒ è§£å†³æ–¹æ¡ˆç”Ÿæˆå¤±è´¥: {e}")
            # é™çº§åˆ°é¡ºåºæ‰§è¡Œ
            solution_texts = []
            for i in range(3):
                try:
                    solution = await self._call_with_simple_retry(
                        self.custom, input=problem, instruction=OPTIMAL_MATH_SOLVE_PROMPT
                    )
                    solution_texts.append(solution['response'])
                except Exception as e:
                    print(f"    è§£å†³æ–¹æ¡ˆ{i+1}ç”Ÿæˆå¤±è´¥: {e}")
                    solution_texts.append(f"Error: {e}")
                
                # é¡ºåºæ‰§è¡Œæ—¶ä¹Ÿå»æ‰å»¶è¿Ÿ
        
        # # ä½¿ç”¨è‡ªä¸€è‡´æ€§é›†æˆ
        # print("  OptimalWorkflow: è¿›è¡Œè‡ªä¸€è‡´æ€§é›†æˆ...")
        # valid_solutions = [s for s in solution_texts if not str(s).startswith("Error")]
        # if not valid_solutions:
        #     print("    âŒ æ²¡æœ‰æœ‰æ•ˆè§£å†³æ–¹æ¡ˆè¿›è¡Œé›†æˆ")
        #     return "OptimalWorkflowæ‰§è¡Œå¤±è´¥", 0.0
            
        try:
            final_solution = await self._call_with_simple_retry(
                self.sc_ensemble, solutions=valid_solutions, problem=problem
            )
            print(f"    âœ… é›†æˆæˆåŠŸ: {final_solution['response'][:80]}...")
        except Exception as e:
            print(f"    âš ï¸  è‡ªä¸€è‡´æ€§é›†æˆå¤±è´¥ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªæœ‰æ•ˆè§£å†³æ–¹æ¡ˆ: {e}")
            final_solution = {'response': valid_solutions[0]}
        
        # ç¼–ç¨‹å™¨éªŒè¯
        print("  OptimalWorkflow: ä»£ç éªŒè¯...")
        try:
            verification = await self._call_with_simple_retry(
                self.programmer, problem=problem, analysis=final_solution['response']
            )
            # æ£€æŸ¥éªŒè¯ç»“æœæ˜¯å¦æœ‰æ•ˆ
            output = verification.get('output', '')
            if output and not str(output).startswith("Error") and str(output).strip():
                print(f"    âœ… éªŒè¯æˆåŠŸï¼Œä½¿ç”¨ä»£ç ç»“æœ: {output}")
                return str(output).strip(), self.llm.get_usage_summary()["total_cost"]
            else:
                print(f"    âš ï¸  éªŒè¯ç»“æœæ— æ•ˆï¼Œä½¿ç”¨è‡ªä¸€è‡´æ€§ç»“æœ: {output}")
        except Exception as e:
            print(f"    âŒ éªŒè¯å¤±è´¥ï¼Œä½¿ç”¨è‡ªä¸€è‡´æ€§ç»“æœ: {e}")
        
        return final_solution['response'], self.llm.get_usage_summary()["total_cost"]

class OverallWorkflow:
    """ä½¿ç”¨ç»¼åˆpromptåœ¨å•æ¬¡è°ƒç”¨ä¸­å®Œæˆæ•´ä¸ªworkflowï¼ŒåŒ…å«ä»£ç æ‰§è¡Œ"""
    def __init__(self, name: str, llm_config, dataset: str, max_retries: int = 3):
        self.name = name
        self.llm_config = llm_config
        self.dataset = dataset
        self.max_retries = max_retries
        self.llm = create_llm_instance(llm_config)
        self.custom = Custom(self.llm)
        # åˆ›å»ºè¿›ç¨‹æ± ç”¨äºä»£ç æ‰§è¡Œ
        self.process_pool = concurrent.futures.ProcessPoolExecutor(max_workers=1)
    
    def __del__(self):
        """ç¡®ä¿è¿›ç¨‹æ± åœ¨å¯¹è±¡é”€æ¯æ—¶å…³é—­"""
        if hasattr(self, 'process_pool'):
            self.process_pool.shutdown(wait=True)
    
    async def exec_code(self, code, timeout=30):
        """å¼‚æ­¥æ‰§è¡Œä»£ç å¹¶åœ¨è¶…æ—¶æ—¶è¿”å›é”™è¯¯"""
        loop = asyncio.get_running_loop()
        
        try:
            # ä½¿ç”¨ç±»çº§åˆ«çš„è¿›ç¨‹æ± 
            future = loop.run_in_executor(self.process_pool, run_code, code)
            # ç­‰å¾…ä»»åŠ¡å®Œæˆæˆ–è¶…æ—¶
            result = await asyncio.wait_for(future, timeout=timeout)
            return result
        except asyncio.TimeoutError:
            future.cancel()
            import gc
            gc.collect()
            return "Error", "Code execution timed out"
        except concurrent.futures.process.BrokenProcessPool:
            # å¦‚æœè¿›ç¨‹æ± æŸåï¼Œé‡æ–°åˆ›å»º
            self.process_pool.shutdown(wait=False)
            self.process_pool = concurrent.futures.ProcessPoolExecutor(max_workers=1)
            return "Error", "Process pool broken, try again"
        except Exception as e:
            return "Error", f"Unknown error: {str(e)}"
    
    def _extract_python_code(self, response: str) -> str:
        """ä»å“åº”ä¸­æå–Pythonä»£ç """
        try:
            import re
            
            # åŒ¹é… ```python ä»£ç å—
            python_blocks = re.findall(r'```python\s*\n(.*?)\n```', response, re.DOTALL)
            if python_blocks:
                return python_blocks[0].strip()
            
            # åŒ¹é…ä¸€èˆ¬çš„ ``` ä»£ç å—
            code_blocks = re.findall(r'```\s*\n(.*?)\n```', response, re.DOTALL)
            for block in code_blocks:
                if 'def solve()' in block:
                    return block.strip()
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»£ç å—ï¼ŒæŸ¥æ‰¾def solve()å‡½æ•°
            solve_match = re.search(r'def solve\(\):.*?(?=\n\n|\n[a-zA-Z]|\Z)', response, re.DOTALL)
            if solve_match:
                return solve_match.group(0).strip()
            
            return None
            
        except Exception as e:
            print(f"ä»£ç æå–é”™è¯¯: {str(e)}")
            return None
    
    def _extract_final_answer(self, response: str, code_result=None) -> str:
        """ä»å“åº”ä¸­æå–æœ€ç»ˆç­”æ¡ˆï¼Œä¼˜å…ˆä½¿ç”¨ä»£ç æ‰§è¡Œç»“æœ"""
        try:
            # å¦‚æœæœ‰ä»£ç æ‰§è¡Œç»“æœï¼Œä¼˜å…ˆä½¿ç”¨
            if code_result is not None:
                import re
                if isinstance(code_result, (int, float)):
                    return str(code_result)
                elif isinstance(code_result, str):
                    numbers = re.findall(r'\d+\.?\d*', str(code_result))
                    if numbers:
                        return numbers[-1]
            
            # æŸ¥æ‰¾ "Final Answer:" éƒ¨åˆ†
            if "Final Answer:" in response:
                lines = response.split('\n')
                for line in lines:
                    if "Final Answer:" in line:
                        import red
                        numbers = re.findall(r'\d+\.?\d*', line)
                        if numbers:
                            return numbers[-1]
            
            # å¦‚æœæ‰¾ä¸åˆ°ï¼Œå°è¯•ä»æœ€åå‡ è¡Œæå–æ•°å­—
            lines = response.strip().split('\n')
            for line in reversed(lines[-5:]):
                import re
                numbers = re.findall(r'\d+\.?\d*', line)
                if numbers:
                    return numbers[-1]
            
            return "Could not extract answer"
            
        except Exception as e:
            return f"Extraction error: {str(e)}"
    
    async def _call_with_simple_retry(self, problem: str):
        """é‡è¯•æœºåˆ¶çš„å†…éƒ¨æ–¹æ³•ï¼ŒåŒ…å«ä»£ç æ‰§è¡Œ"""
        for attempt in range(self.max_retries):
            try:
                if attempt > 0:
                    await asyncio.sleep(0.1)  # é‡è¯•æ—¶çŸ­æš‚å»¶è¿Ÿ
                
                # è°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆå“åº”
                result = await self.custom(input=problem, instruction=OVERALL_WORKFLOW_PROMPT)
                response = result['response']
                
                # æå–å¹¶æ‰§è¡ŒPythonä»£ç 
                python_code = self._extract_python_code(response)
                code_result = None
                
                if python_code:
                    print(f"    ğŸ æ‰§è¡ŒPythonéªŒè¯ä»£ç ...")
                    status, output = await self.exec_code(python_code)
                    if status == "Success":
                        code_result = output
                        print(f"    âœ… ä»£ç æ‰§è¡ŒæˆåŠŸ: {output}")
                    else:
                        print(f"    âŒ ä»£ç æ‰§è¡Œå¤±è´¥: {output}")
                else:
                    print(f"    âš ï¸  æœªæ‰¾åˆ°Pythonä»£ç ")
                
                # æå–æœ€ç»ˆç­”æ¡ˆï¼ˆä¼˜å…ˆä½¿ç”¨ä»£ç ç»“æœï¼‰
                final_answer = self._extract_final_answer(response, code_result)
                
                return final_answer, self.llm.get_usage_summary()["total_cost"]
                
            except Exception as e:
                error_msg = str(e).lower()
                if any(keyword in error_msg for keyword in ['connection', 'timeout', 'network', 'rate limit']):
                    if attempt < self.max_retries - 1:
                        print(f"    âš ï¸  APIé”™è¯¯ï¼Œé‡è¯•ä¸­: {e}")
                        continue
                raise e
        raise Exception("æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†")
    
    async def __call__(self, problem: str):
        """ä¸»è¦çš„è°ƒç”¨æ–¹æ³•"""
        print(f"  OverallWorkflow: ä½¿ç”¨ç»¼åˆprompt+ä»£ç éªŒè¯å¤„ç†é—®é¢˜...")
        return await self._call_with_simple_retry(problem)

class OptimalWorkflowExperiment:
    """OptimalWorkflowå®éªŒç®¡ç†å™¨ - ä»˜è´¹APIç‰ˆæœ¬"""
    
    def __init__(self, data_file: str, results_dir: str = "results", llm_config: str = "meta-llama/llama-3-70b-instruct"):
        self.data_file = data_file
        self.results_dir = Path(results_dir)
        self.results_dir.mkdir(parents=True, exist_ok=True)
        
        # é…ç½®æ¨¡å‹
        self.models_config = LLMsConfig.default()
        self.llm_config = self.models_config.get(llm_config)
        
        # åˆ›å»ºbenchmark
        self.benchmark = GSM8KBenchmark(
            name="GSM8K_OptimalWorkflow", 
            file_path="data/datasets/gsm8k_test.jsonl",
            log_path="workspace/GSM8K_optimal"
        )
        
        # å®éªŒè®°å½•
        self.experiment_log = {
            "start_time": datetime.now().isoformat(),
            "config": {
                "model": self.llm_config.model,
                "data_file": data_file,
                "api_limits": "RPM=100, RPD=50000, TPM=30000 (ä»˜è´¹API)",
                "total_samples": "å›°éš¾æ ·ä¾‹ (25ä¸ªé”™è¯¯æ¡ˆä¾‹)",
                "max_retries": 3,
                "optimization": "æµ‹è¯•å•promptç»¼åˆworkflow vs å¤šæ­¥éª¤workflow",
                "method": "å¯¹æ¯”IO, OptimalWorkflow, OverallWorkflow"
            },
            "results": {},
            "failed_samples": []
        }
    
    async def load_test_data(self) -> List[Dict]:
        """åŠ è½½æµ‹è¯•æ•°æ®"""
        print(f"ğŸ“‚ åŠ è½½æµ‹è¯•æ•°æ®: {self.data_file}")
        
        data = []
        with open(self.data_file, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    data.append(json.loads(line.strip()))
        
        print(f"ğŸ“Š åŠ è½½äº† {len(data)} ä¸ªæµ‹è¯•æ ·æœ¬")
        if len(data) != 200:
            print(f"âš ï¸  è­¦å‘Š: æœŸæœ›200ä¸ªæ ·æœ¬ï¼Œå®é™…åŠ è½½äº†{len(data)}ä¸ª")
        return data
    
    async def run_io_with_optimal_prompt(self, test_data: List[Dict]) -> Dict:
        """è¿è¡ŒIO_with_optimal_promptæµ‹è¯•"""
        method_name = "IO_with_optimal_prompt"
        print(f"\nğŸš€ å¼€å§‹æµ‹è¯•æ–¹æ³•: {method_name}")
        print("=" * 60)
        
        # åˆ›å»ºæ–¹æ³•å®ä¾‹
        method = IO_with_optimal_prompt(
            name=f"GSM8K_{method_name}", 
            llm_config=self.llm_config, 
            dataset="GSM8K",
            max_retries=3
        )
        
        results = []
        failed_samples = []
        start_time = time.time()
        
        for i, sample in enumerate(test_data):
            sample_start_time = time.time()
            try:
                print(f"ğŸ”„ [IO_with_optimal_prompt] å¤„ç†æ ·æœ¬ {i+1}/{len(test_data)}: {sample['question'][:50]}...")
                
                # è°ƒç”¨æ–¹æ³•
                prediction, cost = await method(sample['question'])
                
                # è¯„ä¼°ç»“æœ
                expected = self.benchmark.extract_number(sample['answer'])
                predicted = self.benchmark.extract_number(prediction)
                score, _ = self.benchmark.calculate_score(expected, predicted)
                
                # è·å–tokenä½¿ç”¨æƒ…å†µ
                usage_summary = method.llm.get_usage_summary()
                
                result = {
                    "sample_id": i,
                    "question": sample['question'],
                    "expected": expected,
                    "predicted": predicted,
                    "prediction_text": prediction,
                    "score": score,
                    "cost": cost,
                    "correct": score > 0.5,
                    "processing_time": time.time() - sample_start_time,
                    "input_tokens": usage_summary.get("total_input_tokens", 0),
                    "output_tokens": usage_summary.get("total_output_tokens", 0),
                    "total_tokens": usage_summary.get("total_tokens", 0)
                }
                results.append(result)
                
                print(f"âœ… [IO_with_optimal_prompt] æ ·æœ¬ {i+1} - å¾—åˆ†: {score:.1f}, æˆæœ¬: ${cost:.6f}, tokens: {result['total_tokens']}, ç”¨æ—¶: {result['processing_time']:.1f}s")
                
            except Exception as e:
                processing_time = time.time() - sample_start_time
                error_msg = str(e)
                
                print(f"âŒ [IO_with_optimal_prompt] æ ·æœ¬ {i+1} æœ€ç»ˆå¤±è´¥: {error_msg}")
                
                failed_sample = {
                    "sample_id": i,
                    "question": sample['question'],
                    "error": error_msg,
                    "processing_time": processing_time
                }
                failed_samples.append(failed_sample)
                
                result = {
                    "sample_id": i,
                    "question": sample['question'],
                    "error": error_msg,
                    "score": 0.0,
                    "cost": 0.0,
                    "correct": False,
                    "processing_time": processing_time
                }
                results.append(result)
            
            # æ¯10ä¸ªæ ·æœ¬æ˜¾ç¤ºè¿›åº¦
            if (i + 1) % 10 == 0:
                successful_results = [r for r in results if 'error' not in r]
                if successful_results:
                    avg_score = sum(r['score'] for r in successful_results) / len(successful_results)
                    total_cost = sum(r['cost'] for r in results)
                    elapsed_time = time.time() - start_time
                    estimated_total_time = elapsed_time * len(test_data) / (i + 1)
                    remaining_time = estimated_total_time - elapsed_time
                    print(f"ğŸ“ˆ [IO_with_optimal_prompt] è¿›åº¦: {i+1}/{len(test_data)}, å‡†ç¡®ç‡: {avg_score:.2f}, æˆæœ¬: ${total_cost:.3f}")
                    print(f"â±ï¸  å·²ç”¨æ—¶: {elapsed_time/60:.1f}åˆ†é’Ÿ, é¢„è®¡å‰©ä½™: {remaining_time/60:.1f}åˆ†é’Ÿ")
        
        # è®¡ç®—æœ€ç»ˆç»Ÿè®¡
        total_time = time.time() - start_time
        successful_results = [r for r in results if 'error' not in r]
        
        if successful_results:
            scores = [r['score'] for r in successful_results]
            costs = [r['cost'] for r in results]
            input_tokens = [r.get('input_tokens', 0) for r in results]
            output_tokens = [r.get('output_tokens', 0) for r in results]
            total_tokens = [r.get('total_tokens', 0) for r in results]
            correct_count = sum(1 for r in successful_results if r['correct'])
            
            method_stats = {
                "method_name": method_name,
                "total_samples": len(results),
                "successful_samples": len(successful_results),
                "failed_samples": len(failed_samples),
                "correct_samples": correct_count,
                "accuracy": correct_count / len(successful_results) if successful_results else 0,
                "overall_success_rate": len(successful_results) / len(results),
                "avg_score": statistics.mean(scores) if scores else 0,
                "score_std": statistics.stdev(scores) if len(scores) > 1 else 0,
                "total_cost": sum(costs),
                "avg_cost_per_sample": statistics.mean(costs) if costs else 0,
                "total_input_tokens": sum(input_tokens),
                "total_output_tokens": sum(output_tokens),
                "total_tokens_sum": sum(total_tokens),
                "avg_input_tokens": statistics.mean(input_tokens) if input_tokens else 0,
                "avg_output_tokens": statistics.mean(output_tokens) if output_tokens else 0,
                "avg_total_tokens": statistics.mean(total_tokens) if total_tokens else 0,
                "total_time_seconds": total_time,
                "total_time_minutes": total_time / 60,
                "avg_time_per_sample": total_time / len(results),
                "detailed_results": results
            }
        else:
            method_stats = {
                "method_name": method_name,
                "total_samples": len(results),
                "successful_samples": 0,
                "failed_samples": len(failed_samples),
                "correct_samples": 0,
                "accuracy": 0,
                "overall_success_rate": 0,
                "avg_score": 0,
                "score_std": 0,
                "total_cost": 0,
                "avg_cost_per_sample": 0,
                "total_input_tokens": 0,
                "total_output_tokens": 0,
                "total_tokens_sum": 0,
                "avg_input_tokens": 0,
                "avg_output_tokens": 0,
                "avg_total_tokens": 0,
                "total_time_seconds": total_time,
                "total_time_minutes": total_time / 60,
                "avg_time_per_sample": total_time / len(results),
                "detailed_results": results
            }
        
        print(f"\nğŸ“Š [IO_with_optimal_prompt] æœ€ç»ˆç»“æœ:")
        print(f"   å‡†ç¡®ç‡: {method_stats['accuracy']:.1%} ({method_stats['correct_samples']}/200)")
        if method_stats['failed_samples'] > 0:
            print(f"   å¤±è´¥æ ·æœ¬: {method_stats['failed_samples']}")
        print(f"   æ€»æˆæœ¬: ${method_stats['total_cost']:.4f}")
        print(f"   æ€»tokenæ•°: {method_stats['total_tokens_sum']:,} (è¾“å…¥: {method_stats['total_input_tokens']:,}, è¾“å‡º: {method_stats['total_output_tokens']:,})")
        print(f"   å¹³å‡token/æ ·æœ¬: {method_stats['avg_total_tokens']:.0f} (è¾“å…¥: {method_stats['avg_input_tokens']:.0f}, è¾“å‡º: {method_stats['avg_output_tokens']:.0f})")
        print(f"   ç”¨æ—¶: {method_stats['total_time_minutes']:.1f}åˆ†é’Ÿ")
        
        return method_stats

    async def run_optimal_workflow(self, test_data: List[Dict]) -> Dict:
        """è¿è¡ŒOptimalWorkflowæµ‹è¯• - æ— å»¶è¿Ÿç‰ˆæœ¬"""
        method_name = "OptimalWorkflow"
        print(f"\nğŸš€ å¼€å§‹æµ‹è¯•æ–¹æ³•: {method_name} (æ— å»¶è¿Ÿç‰ˆæœ¬)")
        print("=" * 60)
        
        # åˆ›å»ºæ–¹æ³•å®ä¾‹
        method = OptimalWorkflow(
            name=f"GSM8K_{method_name}", 
            llm_config=self.llm_config, 
            dataset="GSM8K",
            max_retries=3
        )
        
        results = []
        failed_samples = []
        start_time = time.time()
        
        # å»æ‰æ ·æœ¬é—´å»¶è¿Ÿ - ç›´æ¥è¿ç»­å¤„ç†
        for i, sample in enumerate(test_data):
            sample_start_time = time.time()
            try:
                print(f"ğŸ”„ [OptimalWorkflow] å¤„ç†æ ·æœ¬ {i+1}/{len(test_data)}: {sample['question'][:50]}...")
                
                # è°ƒç”¨æ–¹æ³•
                prediction, cost = await method(sample['question'])
                
                # è¯„ä¼°ç»“æœ
                expected = self.benchmark.extract_number(sample['answer'])
                predicted = self.benchmark.extract_number(prediction)
                score, _ = self.benchmark.calculate_score(expected, predicted)
                
                # è·å–tokenä½¿ç”¨æƒ…å†µ
                usage_summary = method.llm.get_usage_summary()
                
                result = {
                    "sample_id": i,
                    "question": sample['question'],
                    "expected": expected,
                    "predicted": predicted,
                    "prediction_text": prediction,
                    "score": score,
                    "cost": cost,
                    "correct": score > 0.5,
                    "processing_time": time.time() - sample_start_time,
                    "input_tokens": usage_summary.get("total_input_tokens", 0),
                    "output_tokens": usage_summary.get("total_output_tokens", 0),
                    "total_tokens": usage_summary.get("total_tokens", 0)
                }
                results.append(result)
                
                print(f"âœ… [OptimalWorkflow] æ ·æœ¬ {i+1} - å¾—åˆ†: {score:.1f}, æˆæœ¬: ${cost:.6f}, tokens: {result['total_tokens']}, ç”¨æ—¶: {result['processing_time']:.1f}s")
                
            except Exception as e:
                processing_time = time.time() - sample_start_time
                error_msg = str(e)
                
                print(f"âŒ [OptimalWorkflow] æ ·æœ¬ {i+1} æœ€ç»ˆå¤±è´¥: {error_msg}")
                
                failed_sample = {
                    "sample_id": i,
                    "question": sample['question'],
                    "error": error_msg,
                    "processing_time": processing_time
                }
                failed_samples.append(failed_sample)
                
                result = {
                    "sample_id": i,
                    "question": sample['question'],
                    "error": error_msg,
                    "score": 0.0,
                    "cost": 0.0,
                    "correct": False,
                    "processing_time": processing_time
                }
                results.append(result)
            
            # å®Œå…¨å»æ‰æ ·æœ¬é—´å»¶è¿Ÿ - ç›´æ¥å¤„ç†ä¸‹ä¸€ä¸ªæ ·æœ¬
            
            # æ¯10ä¸ªæ ·æœ¬æ˜¾ç¤ºè¿›åº¦
            if (i + 1) % 10 == 0:
                successful_results = [r for r in results if 'error' not in r]
                if successful_results:
                    avg_score = sum(r['score'] for r in successful_results) / len(successful_results)
                    total_cost = sum(r['cost'] for r in results)
                    elapsed_time = time.time() - start_time
                    estimated_total_time = elapsed_time * len(test_data) / (i + 1)
                    remaining_time = estimated_total_time - elapsed_time
                    print(f"ğŸ“ˆ [OptimalWorkflow] è¿›åº¦: {i+1}/{len(test_data)}, å‡†ç¡®ç‡: {avg_score:.2f}, æˆæœ¬: ${total_cost:.3f}")
                    print(f"â±ï¸  å·²ç”¨æ—¶: {elapsed_time/60:.1f}åˆ†é’Ÿ, é¢„è®¡å‰©ä½™: {remaining_time/60:.1f}åˆ†é’Ÿ")
        
        # è®¡ç®—æœ€ç»ˆç»Ÿè®¡
        total_time = time.time() - start_time
        successful_results = [r for r in results if 'error' not in r]
        
        if successful_results:
            scores = [r['score'] for r in successful_results]
            costs = [r['cost'] for r in results]
            correct_count = sum(1 for r in successful_results if r['correct'])
            
            method_stats = {
                "method_name": method_name,
                "total_samples": len(results),
                "successful_samples": len(successful_results),
                "failed_samples": len(failed_samples),
                "correct_samples": correct_count,
                "accuracy": correct_count / len(successful_results) if successful_results else 0,
                "overall_success_rate": len(successful_results) / len(results),
                "avg_score": statistics.mean(scores) if scores else 0,
                "score_std": statistics.stdev(scores) if len(scores) > 1 else 0,
                "total_cost": sum(costs),
                "avg_cost_per_sample": statistics.mean(costs) if costs else 0,
                "total_time_seconds": total_time,
                "total_time_minutes": total_time / 60,
                "avg_time_per_sample": total_time / len(results),
                "detailed_results": results
            }
        else:
            method_stats = {
                "method_name": method_name,
                "total_samples": len(results),
                "successful_samples": 0,
                "failed_samples": len(failed_samples),
                "correct_samples": 0,
                "accuracy": 0,
                "overall_success_rate": 0,
                "avg_score": 0,
                "score_std": 0,
                "total_cost": 0,
                "avg_cost_per_sample": 0,
                "total_time_seconds": total_time,
                "total_time_minutes": total_time / 60,
                "avg_time_per_sample": total_time / len(results),
                "detailed_results": results
            }
        
        # è®°å½•å¤±è´¥çš„æ ·æœ¬
        if failed_samples:
            self.experiment_log["failed_samples"] = failed_samples
        
        print(f"\nğŸ“Š [OptimalWorkflow] æœ€ç»ˆç»“æœ:")
        print(f"   å‡†ç¡®ç‡: {method_stats['accuracy']:.1%} ({method_stats['correct_samples']}/200)")
        if method_stats['failed_samples'] > 0:
            print(f"   å¤±è´¥æ ·æœ¬: {method_stats['failed_samples']}")
        print(f"   æ€»æˆæœ¬: ${method_stats['total_cost']:.4f}")
        print(f"   ç”¨æ—¶: {method_stats['total_time_minutes']:.1f}åˆ†é’Ÿ")
        
        return method_stats

    async def run_overall_workflow(self, test_data: List[Dict]) -> Dict:
        """è¿è¡ŒOverallWorkflowæµ‹è¯• - å•promptç»¼åˆç‰ˆæœ¬"""
        method_name = "OverallWorkflow"
        print(f"\nğŸš€ å¼€å§‹æµ‹è¯•æ–¹æ³•: {method_name} (å•promptç»¼åˆç‰ˆæœ¬)")
        print("=" * 60)
        
        # åˆ›å»ºæ–¹æ³•å®ä¾‹
        method = OverallWorkflow(
            name=f"GSM8K_{method_name}", 
            llm_config=self.llm_config, 
            dataset="GSM8K",
            max_retries=3
        )
        
        results = []
        failed_samples = []
        start_time = time.time()
        
        for i, sample in enumerate(test_data):
            sample_start_time = time.time()
            try:
                print(f"ğŸ”„ [OverallWorkflow] å¤„ç†æ ·æœ¬ {i+1}/{len(test_data)}: {sample['question'][:50]}...")
                
                # è°ƒç”¨æ–¹æ³•
                prediction, cost = await method(sample['question'])
                
                # è¯„ä¼°ç»“æœ
                expected = self.benchmark.extract_number(sample['answer'])
                predicted = self.benchmark.extract_number(prediction)
                score, _ = self.benchmark.calculate_score(expected, predicted)
                
                # è·å–tokenä½¿ç”¨æƒ…å†µ
                usage_summary = method.llm.get_usage_summary()
                
                result = {
                    "sample_id": i,
                    "question": sample['question'],
                    "expected": expected,
                    "predicted": predicted,
                    "prediction_text": prediction,
                    "score": score,
                    "cost": cost,
                    "correct": score > 0.5,
                    "processing_time": time.time() - sample_start_time,
                    "total_input_tokens": usage_summary.get("total_input_tokens", 0),
                    "total_output_tokens": usage_summary.get("total_output_tokens", 0),
                    "total_tokens": usage_summary.get("total_tokens", 0)
                }
                results.append(result)
                
                print(f"âœ… [OverallWorkflow] æ ·æœ¬ {i+1} - å¾—åˆ†: {score:.1f}, æˆæœ¬: ${cost:.6f}, tokens: {result['total_tokens']}, ç”¨æ—¶: {result['processing_time']:.1f}s")
                
            except Exception as e:
                processing_time = time.time() - sample_start_time
                error_msg = str(e)
                
                print(f"âŒ [OverallWorkflow] æ ·æœ¬ {i+1} æœ€ç»ˆå¤±è´¥: {error_msg}")
                
                failed_sample = {
                    "sample_id": i,
                    "question": sample['question'],
                    "error": error_msg,
                    "processing_time": processing_time
                }
                failed_samples.append(failed_sample)
                
                result = {
                    "sample_id": i,
                    "question": sample['question'],
                    "error": error_msg,
                    "score": 0.0,
                    "cost": 0.0,
                    "correct": False,
                    "processing_time": processing_time,
                    "total_input_tokens": 0,
                    "total_output_tokens": 0,
                    "total_tokens": 0
                }
                results.append(result)
            
            # æ¯10ä¸ªæ ·æœ¬æ˜¾ç¤ºè¿›åº¦
            if (i + 1) % 10 == 0:
                successful_results = [r for r in results if 'error' not in r]
                if successful_results:
                    avg_score = sum(r['score'] for r in successful_results) / len(successful_results)
                    total_cost = sum(r['cost'] for r in results)
                    elapsed_time = time.time() - start_time
                    estimated_total_time = elapsed_time * len(test_data) / (i + 1)
                    remaining_time = estimated_total_time - elapsed_time
                    print(f"ğŸ“ˆ [OverallWorkflow] è¿›åº¦: {i+1}/{len(test_data)}, å‡†ç¡®ç‡: {avg_score:.2f}, æˆæœ¬: ${total_cost:.3f}")
                    print(f"â±ï¸  å·²ç”¨æ—¶: {elapsed_time/60:.1f}åˆ†é’Ÿ, é¢„è®¡å‰©ä½™: {remaining_time/60:.1f}åˆ†é’Ÿ")
        
        # è®¡ç®—æœ€ç»ˆç»Ÿè®¡
        total_time = time.time() - start_time
        successful_results = [r for r in results if 'error' not in r]
        
        if successful_results:
            scores = [r['score'] for r in successful_results]
            costs = [r['cost'] for r in results]
            correct_count = sum(1 for r in successful_results if r['correct'])
            
            method_stats = {
                "method_name": method_name,
                "total_samples": len(results),
                "successful_samples": len(successful_results),
                "failed_samples": len(failed_samples),
                "correct_samples": correct_count,
                "accuracy": correct_count / len(successful_results) if successful_results else 0,
                "overall_success_rate": len(successful_results) / len(results),
                "avg_score": statistics.mean(scores) if scores else 0,
                "score_std": statistics.stdev(scores) if len(scores) > 1 else 0,
                "total_cost": sum(costs),
                "avg_cost_per_sample": statistics.mean(costs) if costs else 0,
                "total_input_tokens": results[-1]['total_input_tokens'],
                "total_output_tokens": results[-1]['total_output_tokens'],
                "total_tokens_sum": results[-1]['total_tokens'],
                "avg_input_tokens": results[-1]['total_input_tokens'] / len(results) if len(results) > 0 else 0,
                "avg_output_tokens": results[-1]['total_output_tokens'] / len(results) if len(results) > 0 else 0,
                "avg_total_tokens": results[-1]['total_tokens'] / len(results) if len(results) > 0 else 0,
                "total_time_seconds": total_time,
                "total_time_minutes": total_time / 60,
                "avg_time_per_sample": total_time / len(results),
                "detailed_results": results
            }
        else:
            method_stats = {
                "method_name": method_name,
                "total_samples": len(results),
                "successful_samples": 0,
                "failed_samples": len(failed_samples),
                "correct_samples": 0,
                "accuracy": 0,
                "overall_success_rate": 0,
                "avg_score": 0,
                "score_std": 0,
                "total_cost": 0,
                "avg_cost_per_sample": 0,
                "total_input_tokens": 0,
                "total_output_tokens": 0,
                "total_tokens_sum": 0,
                "avg_input_tokens": 0,
                "avg_output_tokens": 0,
                "avg_total_tokens": 0,
                "total_time_seconds": total_time,
                "total_time_minutes": total_time / 60,
                "avg_time_per_sample": total_time / len(results),
                "detailed_results": results
            }
        
        # è®°å½•å¤±è´¥çš„æ ·æœ¬
        if failed_samples:
            self.experiment_log["failed_samples"] = failed_samples
        
        print(f"\nğŸ“Š [OverallWorkflow] æœ€ç»ˆç»“æœ:")
        print(f"   å‡†ç¡®ç‡: {method_stats['accuracy']:.1%} ({method_stats['correct_samples']}/{len(test_data)})")
        if method_stats['failed_samples'] > 0:
            print(f"   å¤±è´¥æ ·æœ¬: {method_stats['failed_samples']}")
        print(f"   æ€»æˆæœ¬: ${method_stats['total_cost']:.4f}")
        print(f"   æ€»tokenæ•°: {method_stats['total_tokens_sum']:,} (è¾“å…¥: {method_stats['total_input_tokens']:,}, è¾“å‡º: {method_stats['total_output_tokens']:,})")
        print(f"   å¹³å‡token/æ ·æœ¬: {method_stats['avg_total_tokens']:.0f} (è¾“å…¥: {method_stats['avg_input_tokens']:.0f}, è¾“å‡º: {method_stats['avg_output_tokens']:.0f})")
        print(f"   ç”¨æ—¶: {method_stats['total_time_minutes']:.1f}åˆ†é’Ÿ")
        
        return method_stats
    
    async def run_experiment(self, method_type: str = "io"):
        """è¿è¡Œå®éªŒ
        
        Args:
            method_type: æ–¹æ³•ç±»å‹ï¼Œ"io" è¡¨ç¤º IO_with_optimal_prompt, "optimal" è¡¨ç¤º OptimalWorkflow, "overall" è¡¨ç¤º OverallWorkflow
        """
        if method_type == "io":
            print("ğŸ¯ å¼€å§‹IO_with_optimal_promptå®éªŒ (å›°éš¾æ ·ä¾‹)")
            print("=" * 60)
            print(f"âœ… æ¨¡å‹é…ç½®: {self.llm_config.model}")
            print(f"ğŸš€ æ–¹æ³•: ç®€å•è¾“å…¥è¾“å‡ºæ¨¡å¼ + æœ€ä¼˜prompt")
            
            # åŠ è½½æµ‹è¯•æ•°æ®
            test_data = await self.load_test_data()
            
            # è¿è¡ŒIO_with_optimal_prompt
            method_stats = await self.run_io_with_optimal_prompt(test_data)
            self.experiment_log["results"] = method_stats
            
            print(f"\nğŸ‰ IO_with_optimal_promptå®éªŒå®Œæˆ! ç»“æœä¿å­˜åœ¨: {self.results_dir}")
            
        elif method_type == "optimal":
            print("ğŸ¯ å¼€å§‹OptimalWorkflowå®éªŒ (å›°éš¾æ ·ä¾‹)")
            print("=" * 60)
            print(f"âœ… æ¨¡å‹é…ç½®: {self.llm_config.model}")
            print(f"ğŸš€ æ–¹æ³•: å®Œæ•´OptimalWorkflow (ç”Ÿæˆ+é›†æˆ+éªŒè¯)")
            
            # åŠ è½½æµ‹è¯•æ•°æ®
            test_data = await self.load_test_data()
            
            # è¿è¡ŒOptimalWorkflow
            method_stats = await self.run_optimal_workflow(test_data)
            self.experiment_log["results"] = method_stats
            
            print(f"\nğŸ‰ OptimalWorkflowå®éªŒå®Œæˆ! ç»“æœä¿å­˜åœ¨: {self.results_dir}")
            
        elif method_type == "overall":
            print("ğŸ¯ å¼€å§‹OverallWorkflowå®éªŒ (å›°éš¾æ ·ä¾‹)")
            print("=" * 60)
            print(f"âœ… æ¨¡å‹é…ç½®: {self.llm_config.model}")
            print(f"ğŸš€ æ–¹æ³•: å•promptç»¼åˆworkflow (ä¸€æ¬¡è°ƒç”¨å®Œæˆæ‰€æœ‰æ­¥éª¤)")
            
            # åŠ è½½æµ‹è¯•æ•°æ®
            test_data = await self.load_test_data()
            
            # è¿è¡ŒOverallWorkflow
            method_stats = await self.run_overall_workflow(test_data)
            self.experiment_log["results"] = method_stats
            
            print(f"\nğŸ‰ OverallWorkflowå®éªŒå®Œæˆ! ç»“æœä¿å­˜åœ¨: {self.results_dir}")
            
        else:
            raise ValueError("method_type å¿…é¡»æ˜¯ 'io', 'optimal' æˆ– 'overall'")
        
        # ä¿å­˜ç»“æœ
        await self.save_results(method_type)
    
    async def save_results(self, method_type: str):
        """ä¿å­˜å®éªŒç»“æœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        method_mapping = {
            "io": "IO_with_optimal_prompt",
            "optimal": "OptimalWorkflow", 
            "overall": "OverallWorkflow"
        }
        method_name = method_mapping.get(method_type, method_type)
        
        # ä¿å­˜è¯¦ç»†ç»“æœåˆ°CSV
        df = pd.DataFrame(self.experiment_log["results"]["detailed_results"])
        csv_file = self.results_dir / f"{method_name}_detailed_results_{timestamp}.csv"
        df.to_csv(csv_file, index=False, encoding='utf-8')
        
        # ä¿å­˜å®Œæ•´å®éªŒæ—¥å¿—
        log_file = self.results_dir / f"{method_name}_experiment_summary_{timestamp}.json"
        with open(log_file, 'w', encoding='utf-8') as f:
            json.dump(self.experiment_log, f, ensure_ascii=False, indent=2)
        
        # å¦‚æœæœ‰å¤±è´¥çš„æ ·æœ¬ï¼Œå•ç‹¬ä¿å­˜
        if self.experiment_log["failed_samples"]:
            failed_samples_file = self.results_dir / f"{method_name}_failed_samples_{timestamp}.json"
            with open(failed_samples_file, 'w', encoding='utf-8') as f:
                json.dump(self.experiment_log["failed_samples"], f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ° {self.results_dir}")
        print(f"ğŸ“ è¯¦ç»†ç»“æœ: {method_name}_{timestamp}.csv")


async def main():
    """ä¸»å‡½æ•°"""
    import sys
    
    # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
    method_type = "overall"  # é»˜è®¤è¿è¡ŒOverallWorkflow
    if len(sys.argv) > 1:
        if sys.argv[1] == "optimal":
            method_type = "optimal"
        elif sys.argv[1] == "io":
            method_type = "io"
        elif sys.argv[1] == "overall":
            method_type = "overall"
        else:
            print("ç”¨æ³•: python single_generate_gsm8k.py [io|optimal|overall]")
            print("  io: è¿è¡Œ IO_with_optimal_prompt")
            print("  optimal: è¿è¡Œ OptimalWorkflow")
            print("  overall: è¿è¡Œ OverallWorkflow (å•promptç»¼åˆç‰ˆæœ¬, é»˜è®¤)")
            return
    
    method_mapping = {
        "io": "IO_with_optimal_prompt",
        "optimal": "OptimalWorkflow", 
        "overall": "OverallWorkflow"
    }
    method_name = method_mapping.get(method_type, method_type)
    print(f"ğŸ¯ GSM8K {method_name}å®éªŒ - å›°éš¾æ ·ä¾‹æµ‹è¯•")
    print("=" * 60)
    
    # åˆ›å»ºå®éªŒ
    experiment = OptimalWorkflowExperiment(
        data_file="z_ablation/200_gsm8k.jsonl",  # ä½¿ç”¨å›°éš¾æ ·ä¾‹æ•°æ®
        results_dir="z_ablation/results",
        llm_config="meta-llama/llama-3-70b-instruct"  # ä½¿ç”¨ç°æœ‰é…ç½®
    )
    
    # è¿è¡Œå®éªŒ
    await experiment.run_experiment(method_type)

if __name__ == "__main__":
    asyncio.run(main())