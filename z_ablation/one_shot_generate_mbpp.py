#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
MBPPæ•°æ®é›† Graph Workflow æµ‹è¯•è„šæœ¬
åŸºäºtest_mbpp_graph.pyçš„æˆåŠŸå®ç°ï¼Œå®Œå…¨å¤åˆ¶graph.pyçš„workflowé€»è¾‘
"""

import asyncio
import json
import os
import sys
import time
import pandas as pd
from pathlib import Path
from typing import Literal, Dict, List, Any
from datetime import datetime
import statistics

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import scripts.operators as operator
from scripts.async_llm import LLMsConfig, create_llm_instance
from benchmarks.mbpp import MBPPBenchmark
import concurrent.futures

DatasetType = Literal["HumanEval", "MBPP", "GSM8K", "MATH", "HotpotQA", "DROP"]

# ä½¿ç”¨å’Œgraph.pyç›¸åŒçš„prompt (ä»åŸå§‹prompt.pyå¤åˆ¶)
CODE_GENERATE_PROMPT = """
Generate a Python function to solve the given problem. Ensure the function name matches the one specified in the problem. Include necessary imports. Use clear variable names and add comments for clarity.

Problem:
"""

FIX_CODE_PROMPT = """
You are a Python expert. Your task is to fix the failed solution based on the provided error information.

Please analyze the error and provide a corrected solution:
"""

# MBPPè‡ªä¸€è‡´æ€§é›†æˆprompt
MBPP_SC_ENSEMBLE_PROMPT = """
Given the programming problem: {question}

Several Python solutions have been generated to address this problem:
{solutions}

Carefully evaluate these solutions and identify the code that is most likely to be correct. 
Consider factors like:
- Code correctness and logic
- Handling of edge cases
- Code quality and efficiency
- Adherence to the problem requirements

In the "thought" field, provide a detailed explanation of your evaluation process.
In the "solution_letter" field, output only the single letter ID (A, B, C, etc.) corresponding to the best solution.
Do not include any additional text or explanation in the "solution_letter" field.
"""



# MBPPç»¼åˆworkflow promptï¼ˆç±»ä¼¼GSM8Kçš„OVERALL_WORKFLOW_PROMPTï¼‰
MBPP_OVERALL_WORKFLOW_PROMPT = """
You are an expert Python programmer with advanced problem-solving capabilities. 
You will solve a programming problem using a comprehensive multi-approach workflow. Follow these steps carefully:

## STEP 1: GENERATE MULTIPLE SOLUTIONS
First, generate THREE different Python solutions to the problem using different approaches:

**Solution A (Direct Implementation):**
- Read and understand the problem requirements thoroughly
- Identify the core algorithm or logic needed
- Implement a straightforward solution
- Ensure proper handling of input/output format
- Add comments explaining the logic

**Solution B (Alternative Approach):**
- Consider a different algorithm or implementation strategy
- Use alternative data structures or methods if applicable
- Focus on efficiency or different edge case handling
- Provide clear code comments

**Solution C (Optimized/Robust Approach):**
- Implement with focus on optimization or robustness
- Consider performance improvements or comprehensive error handling
- Use advanced Python features if beneficial
- Ensure code quality and readability

## STEP 2: SELF-CONSISTENCY EVALUATION
Carefully evaluate the three solutions above:
- Compare the logic and implementation quality of each solution
- Identify which solution best addresses the problem requirements
- Check for potential bugs or edge case issues
- Consider code efficiency and readability
- Select the most reliable and well-implemented solution

## STEP 3: CODE VERIFICATION
Test your selected solution mentally or with examples:
- Walk through the code with sample inputs
- Verify the logic handles edge cases properly
- Ensure the function signature matches requirements
- Check that the return type and format are correct

## STEP 4: FINAL SOLUTION
Provide your final, best solution:
- Use the most reliable solution from your analysis
- Ensure it's complete and ready to run
- Include proper function signature as specified
- Add brief comments if helpful

## FORMAT YOUR RESPONSE AS:

### Solution A:
```python
# Your first solution here
```

### Solution B:
```python
# Your second solution here
```

### Solution C:
```python
# Your third solution here
```

### Self-Consistency Evaluation:
[Compare the three solutions and select the best one]

### Final Solution:
```python
# Your final, best solution here
```

Now solve this programming problem:

"""

class TokenTracker:
    """Token ä½¿ç”¨ç»Ÿè®¡å™¨"""
    def __init__(self):
        self.reset()

    def reset(self):
        self.calls = []
        self.total_input_tokens = 0
        self.total_output_tokens = 0
        self.total_tokens = 0
        self.total_cost = 0.0

    def add_call(self, operation: str, input_tokens: int, output_tokens: int, cost: float):
        self.calls.append({
            "operation": operation,
            "input_tokens": input_tokens,
            "output_tokens": output_tokens,
            "total_tokens": input_tokens + output_tokens,
            "cost": cost,
        })
        self.total_input_tokens += input_tokens
        self.total_output_tokens += output_tokens
        self.total_tokens += input_tokens + output_tokens
        self.total_cost += cost

    def get_summary(self):
        return {
            "total_calls": len(self.calls),
            "total_input_tokens": self.total_input_tokens,
            "total_output_tokens": self.total_output_tokens,
            "total_tokens": self.total_tokens,
            "total_cost": self.total_cost,
            "calls_detail": self.calls,
        }

class Vanilla_Workflow:
    """å®Œå…¨å¤åˆ¶graph.pyçš„Workflowç±»"""
    def __init__(
        self,
        name: str,
        llm_config,
        dataset: DatasetType,
    ) -> None:
        self.name = name
        self.dataset = dataset
        self.llm = create_llm_instance(llm_config)
        self.custom = operator.Custom(self.llm)
        self.custom_code_generate = operator.CustomCodeGenerate(self.llm)
        self.test = operator.Test(self.llm)
        self.sc_ensemble = operator.ScEnsemble(self.llm)
        
        # æ·»åŠ tokenè¿½è¸ªå™¨
        self.token_tracker = TokenTracker()

    def _get_tokens_and_cost(self):
        """è·å–å½“å‰çš„tokenså’Œcost"""
        if hasattr(self.llm, 'usage_tracker'):
            return (
                self.llm.usage_tracker.total_input_tokens,
                self.llm.usage_tracker.total_output_tokens, 
                self.llm.usage_tracker.total_cost
            )
        else:
            return 0, 0, 0.0
    
    def _track_operation(self, operation: str, initial_input: int, initial_output: int, initial_cost: float):
        """è¿½è¸ªä¸€æ¬¡æ“ä½œçš„tokenä½¿ç”¨"""
        current_input, current_output, current_cost = self._get_tokens_and_cost()
        delta_input = current_input - initial_input
        delta_output = current_output - initial_output
        delta_cost = current_cost - initial_cost
        
        self.token_tracker.add_call(operation, delta_input, delta_output, delta_cost)
        return delta_input, delta_output, delta_cost

    async def __call__(self, problem: str, entry_point: str):
        """
        Implementation of the graph - å®Œå…¨å¤åˆ¶åŸå§‹graph.py
        Custom operator to generate anything you want.
        But when you want to get standard code, you should use custom_code_generate operator.
        """
        print(f"    ğŸ”„ å¼€å§‹Graph workflow: {entry_point}")
        
        # é‡ç½®tokenè¿½è¸ªå™¨
        self.token_tracker.reset()
        
        # ç¬¬1-3æ¬¡è°ƒç”¨: Generate 3 solutions
        print(f"    ğŸ“ ç”Ÿæˆ3ä¸ªè§£å†³æ–¹æ¡ˆ...")
        solutions = [] 
        for i in range(3):
            initial_input, initial_output, initial_cost = self._get_tokens_and_cost()
            
            solution = await self.custom_code_generate(problem=problem, entry_point=entry_point, instruction=CODE_GENERATE_PROMPT)
            # CustomCodeGenerate returns {'code': '...'}, not {'response': '...'}
            solutions.append(solution['code'])
            
            # è¿½è¸ªæ­¤æ¬¡è°ƒç”¨
            self._track_operation(f"CustomCodeGenerate_{i+1}", initial_input, initial_output, initial_cost)
            print(f"      âœ… è§£å†³æ–¹æ¡ˆ {i+1} ç”Ÿæˆå®Œæˆ")
        
        # ç¬¬4æ¬¡è°ƒç”¨: Self-consistency ensemble
        print(f"    ğŸ”„ è‡ªä¸€è‡´æ€§é›†æˆ...")
        initial_input, initial_output, initial_cost = self._get_tokens_and_cost()
        
        best_solution = await self.sc_ensemble(solutions=solutions, problem=problem)
        
        # è¿½è¸ªScEnsembleè°ƒç”¨
        self._track_operation("ScEnsemble", initial_input, initial_output, initial_cost)
        
        # Ensure sc_ensemble returned correct format
        if not isinstance(best_solution, dict) or 'response' not in best_solution:
            # Fallback to first solution
            best_solution = {"response": solutions[0]}
        
        # Test the solution
        print(f"    ğŸ§ª æµ‹è¯•è§£å†³æ–¹æ¡ˆ...")
        try:
            test_result = await self.test(problem=problem, solution=best_solution['response'], entry_point=entry_point)
        except Exception as e:
            print(f"    âš ï¸ æµ‹è¯•è¿‡ç¨‹å‡ºç°å¼‚å¸¸: {str(e)}")
            # If test fails, return the solution without testing
            test_result = {"result": False, "solution": best_solution['response']}
        
        if test_result['result']:
            print(f"    âœ… æµ‹è¯•é€šè¿‡!")
            tokens_info = {
                "input_tokens": self.token_tracker.total_input_tokens,
                "output_tokens": self.token_tracker.total_output_tokens,
                "total_tokens": self.token_tracker.total_tokens,
            }
            return test_result['solution'], tokens_info, self.llm.usage_tracker.total_cost
        else:
            # ç¬¬5æ¬¡è°ƒç”¨: If the test fails, try to fix the solution
            print(f"    ğŸ”§ æµ‹è¯•å¤±è´¥ï¼Œå°è¯•ä¿®å¤è§£å†³æ–¹æ¡ˆ...")
            try:
                initial_input, initial_output, initial_cost = self._get_tokens_and_cost()
                
                fixed_solution = await self.custom(input=f"Problem: {problem}\nFailed solution: {best_solution['response']}\nError: {test_result['solution']}", instruction=FIX_CODE_PROMPT)
                
                # è¿½è¸ªCustomä¿®å¤è°ƒç”¨
                self._track_operation("Custom_Fix", initial_input, initial_output, initial_cost)
                
                print(f"    âœ… ä¿®å¤å®Œæˆ")
                tokens_info = {
                    "input_tokens": self.token_tracker.total_input_tokens,
                    "output_tokens": self.token_tracker.total_output_tokens,
                    "total_tokens": self.token_tracker.total_tokens,
                }
                return fixed_solution['response'], tokens_info, self.llm.usage_tracker.total_cost
            except Exception as e:
                print(f"    âŒ ä¿®å¤å¤±è´¥: {str(e)}")
                # If fix also fails, return the original solution
                tokens_info = {
                    "input_tokens": self.token_tracker.total_input_tokens,
                    "output_tokens": self.token_tracker.total_output_tokens,
                    "total_tokens": self.token_tracker.total_tokens,
                }
                return best_solution['response'], tokens_info, self.llm.usage_tracker.total_cost

class MBPPExperiment:
    """MBPPå®éªŒç®¡ç†å™¨"""
    
    def __init__(self, data_file: str, results_dir: str = "results", llm_config: str = "openai/gpt-4o-mini"):
        self.data_file = data_file
        self.results_dir = Path(results_dir)
        self.results_dir.mkdir(exist_ok=True)
        
        # é…ç½®æ¨¡å‹
        self.models_config = LLMsConfig.default()
        self.llm_config = self.models_config.get(llm_config)
        
        if self.llm_config is None:
            raise ValueError(f"Model '{llm_config}' not found in configuration")
        
        # åˆ›å»ºbenchmarkå®ä¾‹
        self.benchmark = MBPPBenchmark("MBPP", data_file, str(self.results_dir))
        
        # ç»“æœå­˜å‚¨
        self.results = {
            "optimal_workflow": {
                "predictions": [],
                "costs": [],
                "tokens": [],
                "times": [],
                "success_rate": 0.0,
                "total_cost": 0.0,
                "avg_tokens": 0.0,
                "total_tokens": 0,  # æ·»åŠ æ€»tokensç»Ÿè®¡
                "avg_time": 0.0
            }
        }
    
    async def load_test_data(self, max_samples: int = None) -> List[Dict]:
        """åŠ è½½æµ‹è¯•æ•°æ®"""
        test_data = []
        with open(self.data_file, 'r', encoding='utf-8') as f:
            for i, line in enumerate(f):
                if max_samples and i >= max_samples:
                    break
                if line.strip():
                    test_data.append(json.loads(line.strip()))
        return test_data

    async def run_optimal_workflow(self, test_data: List[Dict]) -> Dict:
        """è¿è¡ŒVanilla_OptimalWorkflowæµ‹è¯•"""
        print(f"\n{'='*60}")
        print(f"ğŸš€ å¼€å§‹è¿è¡ŒVanilla_OptimalWorkflowæµ‹è¯•")
        print(f"ğŸ“Š æµ‹è¯•æ ·æœ¬æ•°é‡: {len(test_data)}")
        print(f"{'='*60}")
        
        workflow = Vanilla_Workflow(
            name="MBPP_Vanilla_OptimalWorkflow", 
            llm_config=self.llm_config, 
            dataset="MBPP"
        )
        
        predictions = []
        costs = []
        tokens_list = []
        times = []
        success_count = 0
        successes = []
        
        for i, sample in enumerate(test_data):
            entry_point = sample.get('entry_point', f'problem_{i}')
            print(f"\nğŸ“ å¤„ç†é—®é¢˜ {i+1}/{len(test_data)}: {entry_point}")
            
            start_time = time.time()
            
            try:
                result, tokens_info, cost = await workflow(sample['prompt'], sample['entry_point'])
                
                processing_time = time.time() - start_time
                
                # ä½¿ç”¨MBPPBenchmarkæ£€æŸ¥ç»“æœæ­£ç¡®æ€§
                ret = self.benchmark.check_solution(result, sample['test'], sample['entry_point'])
                success = ret[0] == self.benchmark.PASS
                successes.append(success)
                
                predictions.append(result)
                costs.append(cost)
                tokens_list.append(tokens_info)
                times.append(processing_time)
                
                if success:
                    success_count += 1
                
                print(f"  âœ… å®Œæˆ - æˆåŠŸ: {'æ˜¯' if success else 'å¦'}")
                print(f"  ğŸ’° æˆæœ¬: ${cost:.6f}")
                print(f"  ğŸ“Š Tokens: {tokens_info['input_tokens']} + {tokens_info['output_tokens']} = {tokens_info['total_tokens']}")
                print(f"  â±ï¸  æ—¶é—´: {processing_time:.2f}s")
                
            except Exception as e:
                processing_time = time.time() - start_time
                print(f"  âŒ é”™è¯¯: {str(e)}")
                
                predictions.append(f"ERROR: {str(e)}")
                costs.append(0.0)
                tokens_list.append({"input_tokens": 0, "output_tokens": 0, "total_tokens": 0})
                times.append(processing_time)
                successes.append(False)
        
        # è®¡ç®—ç»Ÿè®¡æ•°æ®
        total_cost = sum(costs)
        total_tokens = sum(t['total_tokens'] for t in tokens_list)  # æ·»åŠ æ€»tokens
        avg_tokens = total_tokens / len(tokens_list) if tokens_list else 0
        avg_time = sum(times) / len(times) if times else 0
        success_rate = success_count / len(test_data) if test_data else 0
        
        print(f"\n{'='*60}")
        print(f"ğŸ“Š Vanilla_OptimalWorkflowæµ‹è¯•å®Œæˆ!")
        print(f"âœ… æˆåŠŸç‡: {success_rate:.2%} ({success_count}/{len(test_data)})")
        print(f"ğŸ’° æ€»æˆæœ¬: ${total_cost:.6f}")
        print(f"ğŸ“ˆ æ€»tokens: {total_tokens:,} (å¹³å‡: {avg_tokens:.0f}/æ ·æœ¬)")
        print(f"â±ï¸  å¹³å‡æ—¶é—´: {avg_time:.2f}s")
        print(f"{'='*60}")
        
        # ä¿å­˜è¯¦ç»†æ—¥å¿—
        log_data = []
        for i, (sample, pred, cost, tokens, time_taken, success) in enumerate(zip(test_data, predictions, costs, tokens_list, times, successes)):
            # ä¿®æ­£individual successåˆ¤æ–­
            individual_success = not pred.startswith("ERROR:") and not pred.startswith("Error:")
            log_entry = {
                "problem_id": i,
                "entry_point": sample.get('entry_point', ''),
                "problem": sample.get('prompt', sample.get('text', '')),
                "prediction": pred,
                "cost": cost,
                "tokens": tokens,
                "time": time_taken,
                "success": individual_success
            }
            log_data.append(log_entry)
        
        # ä¿å­˜æ—¥å¿—
        log_file = self.results_dir / "Vanilla_optimal_workflow_log.jsonl"
        with open(log_file, 'w', encoding='utf-8') as f:
            for entry in log_data:
                f.write(json.dumps(entry, ensure_ascii=False) + '\n')
        
        return {
            "predictions": predictions,
            "costs": costs,
            "tokens": tokens_list,
            "times": times,
            "success_rate": success_rate,
            "total_cost": total_cost,
            "avg_tokens": avg_tokens,
            "total_tokens": total_tokens,  # æ·»åŠ æ€»tokens
            "avg_time": avg_time,
            "log_file": str(log_file)
        }

    # æ³¨é‡Šæ‰OverallWorkflowç›¸å…³æ–¹æ³•
    # async def run_overall_workflow(self, test_data: List[Dict]) -> Dict:
    #     """è¿è¡ŒOverallWorkflowæµ‹è¯•"""
    #     pass

    async def run_experiment(self, method_type: str = "optimal", max_samples: int = None):
        """è¿è¡Œå®éªŒ"""
        print(f"ğŸ¯ åŠ è½½æµ‹è¯•æ•°æ®...")
        test_data = await self.load_test_data(max_samples)
        
        if method_type == "optimal":
            self.results["optimal_workflow"] = await self.run_optimal_workflow(test_data)
        # elif method_type == "overall":
        #     self.results["overall_workflow"] = await self.run_overall_workflow(test_data)
        else:
            raise ValueError(f"ç›®å‰åªæ”¯æŒ optimal æ–¹æ³•ï¼Œä¸æ”¯æŒ: {method_type}")
    
    async def save_results(self, method_type: str):
        """ä¿å­˜å®éªŒç»“æœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        summary = {
            "experiment_info": {
                "data_file": str(self.data_file),
                "method_type": method_type,
                "timestamp": timestamp,
                "llm_config": str(self.llm_config.model)
            },
            "results": {}
        }
        
        for key, result in self.results.items():
            if result["predictions"]:  # åªä¿å­˜æœ‰æ•°æ®çš„ç»“æœ
                summary["results"][key] = {
                    "success_rate": result["success_rate"],
                    "total_cost": result["total_cost"],
                    "avg_tokens": result["avg_tokens"],
                    "total_tokens": result["total_tokens"],  # æ·»åŠ æ€»tokens
                    "avg_time": result["avg_time"],
                    "sample_count": len(result["predictions"])
                }
        
        # ä¿å­˜æ‘˜è¦
        summary_file = self.results_dir / f"mbpp_{method_type}_summary_{timestamp}.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {summary_file}")

async def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='MBPP Vanilla_OptimalWorkflowæµ‹è¯•')
    parser.add_argument('--data_file', default='data/datasets/mbpp_test.jsonl', help='æµ‹è¯•æ•°æ®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--method', choices=['optimal'], default='optimal', help='æµ‹è¯•æ–¹æ³•(ç›®å‰åªæ”¯æŒoptimal)')
    parser.add_argument('--max_samples', type=int, default=None, help='æœ€å¤§æµ‹è¯•æ ·æœ¬æ•°')
    parser.add_argument('--llm_config', default='openai/gpt-4o-mini', help='LLMé…ç½®')
    
    args = parser.parse_args()
    
    print(f"ğŸ¯ MBPP Vanilla_OptimalWorkflow æµ‹è¯•")
    print(f"ğŸ“‚ æ•°æ®æ–‡ä»¶: {args.data_file}")
    print(f"ğŸ”§ æ–¹æ³•: {args.method}")
    print(f"ğŸ”¢ æœ€å¤§æ ·æœ¬æ•°: {args.max_samples}")
    print(f"ğŸ¤– LLMé…ç½®: {args.llm_config}")
    
    # åˆ›å»ºå®éªŒ
    experiment = MBPPExperiment(
        data_file=args.data_file,
        results_dir="z_ablation/results/mbpp",
        llm_config=args.llm_config
    )
    
    # è¿è¡Œå®éªŒ
    await experiment.run_experiment(method_type=args.method, max_samples=args.max_samples)
    
    # ä¿å­˜ç»“æœ
    await experiment.save_results(args.method)

if __name__ == "__main__":
    asyncio.run(main())